{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "590e74322e445b4504ce89a86fecd2d1",
     "grade": false,
     "grade_id": "cell-880d58d7d89bcc50",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Discrete Choice\n",
    "\n",
    "Discrete choice models are models that model a single (mutually exclusive) choice, in contrast to the standard models where a quantity is estimated. \n",
    "\n",
    "In this notebook we will try to get you familiarized with discrete choice, the difference between logit and probit, and how to implement them (and more advanced models), using the module [statsmodels](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "### Linear Regression\n",
    "As a lazy student we want to study as little as possible, but still pass the final test. Let's pretend we have a dataset of last year's students, with the hours they studied for the test, and whether or not they passed. From this dataset we can make an estimation how many hours we have to study ourselves to pass. If we would try a linear expression approach we would try to fit the function: \n",
    "\n",
    "\\begin{equation}\n",
    " Y = \\beta_0 + \\beta_1 X_1\n",
    "\\end{equation}\n",
    "\n",
    "where $Y$ equals the chance of passing, $\\beta_0$ the base chance of passing, $\\beta_1$ the increase of chance of passing per hour we study, and $X_1$ the hours a student studied.\n",
    "\n",
    "First: We install statsmodels and test if we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90459be93a2cbcb23daa3c066304faaf",
     "grade": false,
     "grade_id": "cell-1e0a3b936effd6a7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import statsmodels\n",
    "clear_output()\n",
    "print(\"Everything imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7de0bfde46d6d8d0181c30a94c6b4915",
     "grade": false,
     "grade_id": "cell-a7358d55e51427e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# the hours each student studied for the test, and whether they passed or failed\n",
    "students = {'hours': [0, 2, 3, 4, 4.5, 4.5, 5, 6, 6.5, 7, 8, 8, 9, 9.5, 10, 10.5, 12, 13.5],\n",
    "            'passed': [False, False, False, False, False, False, False, False, False, True, True, True, False, True, True, False, True, True]}\n",
    "\n",
    "# use ordinary least squares (OLS) to fit our function Y\n",
    "intercept, slope = sm.OLS(students['passed'], \n",
    "                          sm.add_constant(students['hours'])).fit().params\n",
    "\n",
    "# plot the results of the students\n",
    "plt.scatter(students['hours'], students['passed'])\n",
    "plt.xlabel('hours studied'); plt.ylabel('pass/fail')\n",
    "\n",
    "# plot the results of the fit\n",
    "x_vals = np.array(plt.gca().get_xlim())\n",
    "y_vals = intercept + slope * x_vals\n",
    "plt.plot(x_vals, y_vals, '--')\n",
    "\n",
    "# set proper axes\n",
    "plt.xlim([-1, 14]); plt.ylim([-0.1, 1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3013bad47987c2b1be8660eb282edace",
     "grade": false,
     "grade_id": "cell-7ae05cb8dc9db673",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We fitted our $Y$ function with a simple linear square approach, by using the method [sm.OLS](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) from statsmodels. Its first argument is the $Y$ value we try to fit, and the second argument are the $\\beta$ values we try to fit. Note that we have to add a constant value (`sm.add_constant`) if we want a $\\beta_0$ value\n",
    "\n",
    "The obvious problem with the linear regression approach is that we try to model the chance of pass (or failure), but our model can give values outside of the range (0, 1). If a student did not study the model gives a chance lower than 0% of succes, and after more than 14 hours of study the chance of passing is higher than a 100%! To solve this problem we need discrete models.\n",
    "\n",
    "### Binary Discrete Choice\n",
    "Discrete models are similar to our previous approach, except the $Y$ value is not modelled on a continuous scale, but is limited between discrete alternatives. To solve these models we need a **utility** function, which closely resembles the function we tried to fit using linear regression, but with some added noise $\\epsilon$: \n",
    "\\begin{equation}\n",
    " U = \\beta_0 + \\beta_1 X_1 + \\epsilon \\\\\n",
    " \\begin{cases}\n",
    " pass & U > 0\\\\\n",
    " fail & else\\\\\n",
    " \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This utility function represents the preference for an outcome. In our case if the utility is a number above zero, it means the student passes, otherwise the student fails. To get a probability from our utility we need a function $F$ which maps the utility to a probability between the range (0, 1).\n",
    "\\begin{equation}\n",
    " P_{n1} = F(U)\n",
    "\\end{equation}\n",
    "\n",
    "Here we will discuss two of the most common $F$ funcitons, **Logit** & **Probit**.\n",
    "\n",
    "### Logit (Logistic regression)\n",
    "When using the Logit approach we assume that the log-odds of pass/failure can be expressed as a linear function of our input (the utility), and our unobserved fraction of the utility ($\\epsilon$) follows a logistic distribution:\n",
    "\\begin{equation}\n",
    " log (\\frac{P_{n1}}{1 - P_{n1}}) = U \\\\\n",
    " \\frac{P_{n1}}{1 - P_{n1}} = e^U\n",
    "\\end{equation} \n",
    "\n",
    "which we can rewrite to:\n",
    "\\begin{equation}\n",
    " P_{n1} = \\frac{e^U}{1 + e^U}\n",
    "\\end{equation} \n",
    "\n",
    "In the Logit case our function $F$ is just the sigmoid/logistic function! \n",
    "\n",
    "So what did we gain from this approach? Our values are now limited between the range (0, 1), but more importantly, we can interpret out coefficients as odds! If for instance after fitting our $\\beta_1$ has a value of $1.1$, it means that for each hour of study the chance of passing  would be $e^{1.1} \\approx 3$ times as likely to happen!\n",
    "\n",
    "### Probit \n",
    "The probit model assumes that the unobserved fraction of the utility ($\\epsilon$) follows a standard normal distribution:\n",
    "\\begin{equation}\n",
    "P_{n1} = \\Phi(\\beta_0 + \\beta_1 X_1)\n",
    "\\end{equation}\n",
    "where $\\Phi$ is the cumulative distribution function of the (standard) normal distribution.\n",
    "\n",
    "### Difference\n",
    "So what is the difference between a normal distribution and a logit distribution? Let's plot them both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8c603ab1f7e837ecb8e4b66ccdc3725",
     "grade": false,
     "grade_id": "cell-cc53c6157e1ef1de",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, logistic\n",
    "import math\n",
    "\n",
    "# standard normal distribution\n",
    "mu = 0; std = 1\n",
    "x = np.linspace(-4, 4, 100)\n",
    "\n",
    "# plot the normal pdf & cdf\n",
    "normal = norm.pdf(x, loc=mu, scale=std)\n",
    "plt.plot(x, normal, label='normal distribution')\n",
    "plt.plot(x, np.cumsum(normal) / sum(normal), label='cumulative normal distribution')\n",
    "\n",
    "# plot the logistic pdf & cdf\n",
    "logist = logistic.pdf(x, loc=mu, scale=std * math.sqrt(3) / math.pi)\n",
    "plt.plot(x, logist, label='logistic distribution')\n",
    "plt.plot(x, np.cumsum(logist) / sum(logist), label='cumulative logistic distribution')\n",
    "\n",
    "plt.ylabel('probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9da83aeeecabf1a9b908194dee36f74",
     "grade": false,
     "grade_id": "cell-33aadf2ff0432bec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "They are very similar! Note that the logit distribution has fatter tails, so it will produce more extreme values than the normal distribution. Now let's see how they differ in performance of the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39e5b26638358c9cca5b4a054f480859",
     "grade": false,
     "grade_id": "cell-4d929ef2ef9064c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the results of the students\n",
    "plt.scatter(students['hours'], students['passed'])\n",
    "plt.xlabel('hours studied'); plt.ylabel('pass/fail')\n",
    "\n",
    "# set proper axes\n",
    "plt.xlim([-1, 14]); plt.ylim([-0.1, 1.1])\n",
    "x_vals = sm.add_constant(np.linspace(-1, 14, 100))\n",
    "\n",
    "# use probit to fit our function\n",
    "probit = sm.Probit(students['passed'], sm.add_constant(students['hours']))\n",
    "pr_model = probit.fit(disp=0)  # disp=0 to silence the verbose function\n",
    "pseudo_r_p = pr_model.prsquared\n",
    "\n",
    "# plot the results of probit\n",
    "y_vals = pr_model.predict(x_vals)\n",
    "plt.plot(x_vals[:, 1], y_vals, '--', label='probit')\n",
    "\n",
    "# use logit to fit our function\n",
    "logit = sm.Logit(students['passed'], sm.add_constant(students['hours']))\n",
    "lo_model = logit.fit(disp=0)  # disp=0 to silence the verbose function\n",
    "pseudo_r_l = lo_model.prsquared\n",
    "\n",
    "# plot the results of logit\n",
    "y_vals = lo_model.predict(x_vals)\n",
    "plt.plot(x_vals[:, 1], y_vals, '--', label='logit')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# show summary of both models\n",
    "print(pr_model.summary())\n",
    "print(lo_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e542ec0ee4cbf68e87b35f77eb68592",
     "grade": false,
     "grade_id": "cell-62977a54520150d2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# so what is the probability of passing the course if you study 9 hours for the test according to the probit model?\n",
    "# your answer has to be correct for at least two significant digits\n",
    "prob = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b3554bd0d5b4c812100a2ed85d1528",
     "grade": true,
     "grade_id": "cell-57b5c18b26749421",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0 <= prob <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b37fa80a53e67817299249d4670a939b",
     "grade": false,
     "grade_id": "cell-f4584947b887f139",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Even though the fitted parameters of both models are quite different, the actual fits are extremely close, and differ little in their predictions, pseudo R squares, or looks.\n",
    "\n",
    "### Multinomial logit\n",
    "When dealing with multiple discrete alternatives, we have to make use of multinomial discrete choice. We rewrite our original utility function into one utility function per choice, where the chance of choice $i$ is defined as such:\n",
    "\\begin{equation}\n",
    "    P_i = Prob(U_i > U_j \\quad \\forall j \\neq i)\n",
    "\\end{equation}\n",
    "\n",
    "We generated a dataset of 250 students, which contains the students' income, distance to university, how lazy they are, and what transport (either bike, car, bus 40 or bus 240) they use to get to university.   \n",
    "\n",
    "Implement multinomial logit yourself, take a look at [MNLogit](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.MNLogit.html). Remember to add a constant (`sm.add_constant`) to our observed variables. Also note that you should use numeric labels, and not the text-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b90fa474c765c4ac2cd50e219489c92e",
     "grade": false,
     "grade_id": "cell-bb623a11d2b61b2b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load our dataset\n",
    "df = pd.read_csv('transport.csv', sep=';', usecols=['income', 'distance', 'lazy', 'transport', 'transport_id'])\n",
    "\n",
    "# print the 'head' of the dataframe to get a feel for the data \n",
    "print(df.head())\n",
    "\n",
    "# implement multinomial logit\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# let's see how it predicts on our own dataset (you should get at least 200 out of 250 predictions correct!)\n",
    "# the predict function returns a dataframe shape (250, 4), where each column is the chance of that choice.\n",
    "# Assume that the option with the highest chance is chosen\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b4b66b6164c0b67d412538e5c973b4",
     "grade": true,
     "grade_id": "cell-7dbdedf27295bd2c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(model).__name__ == 'MultinomialResultsWrapper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03e6cdc349e7206b62981e348ae8a44d",
     "grade": false,
     "grade_id": "cell-0f7057df8f77da51",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Logit limitations\n",
    "- **Taste variation**: every students shares the same $\\beta$ values, while this not necessarily has to be true. Some students might've done earlier courses which resemble a lot of the subject matter of the course, so they have a higher $\\beta_0$ value, and some student might just be more efficient while learning, resulting in a higher $\\beta_1$ value. Logit does not allow different $\\beta$ values for its choice makers.\n",
    "- **Independece of Irrelevant Alternatives (IIA)**: If we make people choose between two options (e.g. bulbasaur and squirtle), adding a third option (charmander) should not change peoples original order of the two options. For example: if someone prefers a squirtle over a bulbasaur, by also giving them the choice of a charmander, they should not suddenly like bulbasaur more. Multinomial logit does not allow independence of irrelevant alternatives.\n",
    "- **Repeated choice**: Logit assumes no correlation in repeated choices. If a person takes the bike to work one day, it might influence him/her to take the bike the next day. Maybe he/she got lost, so won't take the bike again. Or the person gets to know the road better, so biking the next day becomes faster.\n",
    "\n",
    "\n",
    "### Nested logit\n",
    "When we look closer at the data we see that bus 240 and bus 40 are similar choices, and after a quick questionnaire we realize that if bus 40 does not go all student's will use bus 240 and vice versa. Multinomial logit violates this bus-dependency (independence of irrelevant alternatives). However if we would implement nested logit we would be guaranteed of this dependency:\n",
    "\n",
    "![...](nested.png)\n",
    "\n",
    "Your task now is to finish the NestedLogit class, which incorporates this logic. It should fit the choices bike, car, and bus using `sm.MNLogit` and the two different buses by `sm.Logit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3a44e6a375bdffcf5a113c53fdaa20",
     "grade": false,
     "grade_id": "cell-4c2ab04ec26426a0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class NestedLogit():\n",
    "    def __init__(self, labels, variables):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Method that fits the predictions of the NestedLogit.\n",
    "        \"\"\"        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, variables):\n",
    "        \"\"\"\n",
    "        Method that returns the predictions of the NestedLogit, based on the fit, shape (N, 4)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "# Calls to NestedLogit\n",
    "nlogit = NestedLogit(df['transport_id'], sm.add_constant(df[['income', 'distance', 'lazy']]))\n",
    "nlogit.fit()\n",
    "y_vals = nlogit.predict(sm.add_constant(df[['income', 'distance', 'lazy']]))\n",
    "# How does nested logit compare to multinomial logit? You should get at least 200 predictions correct! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7196435cff9121e1b780f5a8516d3f7a",
     "grade": true,
     "grade_id": "cell-ae0095cf5f0b72cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "nlogit = NestedLogit(df['transport_id'], sm.add_constant(df[['income', 'distance', 'lazy']]))\n",
    "nlogit.fit()\n",
    "y_vals = nlogit.predict(sm.add_constant(df[['income', 'distance', 'lazy']]))\n",
    "assert y_vals.shape == (250, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa485a9f68e1e1903d3b50098304eac5",
     "grade": false,
     "grade_id": "cell-949d6ee2cc5b3be2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the file generate_data.py is the data generated. Can you design a dataset where NestedLogit outperforms MultiNomialLogit? Why does nested logit not outperform multinomial logit?\n",
    "\n",
    "### Advanced models\n",
    "For more complex logit models, such as mixed logit which allows for taste variation. Take a look at [PyLogit](https://github.com/timothyb0912/pylogit)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
